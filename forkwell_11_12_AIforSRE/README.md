# テーマ
OpenAIの躍進により、我々の仕事はAIとはきっても切り離せない時代となりました。私は正直10年前にこうなることをまるで予想できませんでした。 AIの発展により、今後加速度的に変化していくであろうエンジニアの仕事について、SREとしての立場から考えてみたいと思います。

# 主張したいこと
　AIの進化によって、エンジニアを取り巻く環境には大きないくつかの変化があったと考えています。その中から3点紹介します。

　1つ目はコーディング作業をAIが支援することにより、技術力を資本とした成果の差がより大きくなった。これはAIで生成した成果物の真贋を確かめるためには、利用者の基礎知識が大きく影響することから、基礎知識が豊富なエンジニアは、それを資本により大きな成果を出せるようになったこと。

　2つ目は現時点では優れたAIを開発するには、巨大資本が必要であり、いくつかの一握りの企業だけが状況を左右できる技術になりつつあり、AIそのものがプラットフォームのような状況になりつつあること。

　3つ目はAIのプラットフォーム化に伴い、事業分析用のSQLを生成するなどのいままでエンジニアの作業領域であったような業務においても、別の職種がオーバラップしつつあること。なお、本件においては1点目の真贋を確かめうるか否かという条件があり、比較的自明な要件にのみ適用されている。

　これらの変化を条件としてみた場合に、話し手の主張としては、エンジニアのやるべきことはそんなに変化はなく、基礎知識を体系的に学び、それを資本にAIを活用し、多くのエンジニアはAIをプラットフォームとして受け入れて、これまでのiOSやAndroid、AWS、GCPと同じようにうまく利用するための技術を生み出していく方向になると考える。また本セッションで述べるAIはLLMを指し、LLMは一般的には確率で正解を導き出すものであることから、そういった技術をどういった領域に適用するのが良いかということについても述べたい。

***

# 大規模言語モデル(LLM)
## LLMとは？
　**LLM（Large Language Model、大規模言語モデル）**は、膨大なテキストデータを用いて自然言語処理タスクを行うために設計された機械学習モデルです。特に、OpenAIが2022年に公開したChatGPTが注目され、大規模言語モデル（LLM）の概念が広く知られるようになりました。

　言語モデルは、与えられた文脈に基づいて次に来る単語やフレーズの出現確率を予測します。たとえば、「今日の天気は」の後に続く言葉として「晴れ」「カレーパン」「月曜日」などの選択肢がある場合、文脈に最も自然な「晴れ」が高い確率で選ばれる仕組みです。この予測は、文脈や文法、論理の整合性を考慮して行われます。

　ChatGPTのようなLLMは、単に単語を並べるだけでなく、文脈を理解し、自然な会話や複雑な質問への回答が可能です。これは、自己回帰的に次の単語を生成し続ける高度なプロセスと膨大な学習データによって実現されています。このため、LLMは、質問応答、文章生成、要約といった多様なタスクでの応用が進んでいます。

## LLMが生んだ変化
　LLMの登場によって、話し手の働き方においても大きな変化が起きました。

### コーディング作業などの自動化
　自然言語で仕様を伝えたり、コードの一部をサンプルとして示すことで、必要な実装やテストを高精度で生成できます。これにより、エンジニアの作業効率が大幅に向上しました。

### 文章の要約や解説
  技術的に難解な文章や膨大な量のテキストも、LLMを使えば高い精度で要約や解説が可能です。これにより、情報のインプット効率が向上し、重要な内容を素早く把握できるようになりました。

### 画像、動画生成
　現在、XなどのSNSでもよく目にしますが、キーワードや文章をもとに、それを表現した画像や動画を自動生成する技術が進んでいます。例えば、OpenAIが公開した[sora](https://openai.com/index/sora/)はその一例で、視覚的なインパクトも話題を呼んでいます。

### バックオフィスの作業効率化
　例えば、事業分析用のSQL生成では、以前はディレクターがエンジニアに依頼してSQLを作成することが多くありました。しかし、ChatGPTやGeminiにスキーマ情報と仕様を伝えるだけで、ある程度のSQLを自動生成できるようになりました。また、GMOペパボでは [@khiroyuki1993](https://x.com/khiroyuki1993) さんが開発した [tbls-ask-agent-slack](https://github.com/kromiii/tbls-ask-agent-slack) を使い、[tbls](https://github.com/k1LoW/tbls) のスキーマ情報を基にSlack上でSQLを自動生成する仕組みが整備されています。

# LLMを利用したサービスの事例
　LLMの進化に伴い、多くのサービスがLLMを取り入れた機能を提供しています。ここでは、その中からいくつかを紹介します。
## コーディングに特化したエディタ
　エンジニアが日々の活動において最も恩恵を受けるであろうエディタについて紹介します。
### Cursor
  Cursorは、Visual Studio Codeをベースに開発されたAI統合エディタです。従来のGitHub Copilotと比べ、より対話的にコードを推薦し、リポジトリのリソースやドキュメントへのアクセスも容易な点が特徴です。
## 推薦システム
　推薦システムも、旧来は機械学習をもちいたものが多かったですが、昨今ではLLMを活用したもの増えてきました。
### SUZURIのスリスリくんチャット
　GMOペパボが提供するSUZURIでは、キャラクター「スリスリくん」と会話しながら、ユーザーに適した商品を提案する機能があります。内部では、RAGを活用し、ユーザーに適切な検索ワードを生成、それに基づいて検索結果を推薦する構造になっています。
## ユーザーの入力補助
  OCRとLLMを組み合わせるケースや、これまで利用者が一から文章を作成していた場面でも、LLMの導入が進んでいます。
### B/43のレシート自動読み取り
　先日公開された、B/43の[レシート自動読み取り](https://b43.jp/news/entry/feature-update-self-input) も内部でLLMを活用していることが推測されます。これは話し手がプレスリリースなどを読んだ推測ですが、OCRで読み取った文字列を、LLMにクエリすることで様々なメタ情報を保管していると考えています。

### カラーミーショップにおける商品紹介コンテンツの自動作成
　GMOペパボが提供するECカートサービス「カラーミーショップ」では、ショップオーナーがSNSなどで商品を紹介する際に利用できる文章を自動生成する[カラーミーAIアシスタント（β）](https://help.shop-pro.jp/hc/ja/articles/15000133055123-%E3%82%AB%E3%83%A9%E3%83%BC%E3%83%9F%E3%83%BCAI%E3%82%A2%E3%82%B7%E3%82%B9%E3%82%BF%E3%83%B3%E3%83%88-%CE%B2)が提供されています。この機能は、商品情報をもとに最適な紹介文を生成します。

# Site Reliability Engineering (SRE)
　このセッションのテーマの1つであるSREについて、その役割や重要性について紹介します。SREは、システムの可用性、パフォーマンス、効率性を保つための専任職であり、エンジニアリングと運用の融合によって組織の信頼性を高める役割を担っています。

## SREとは？
　SREはGoogleが提唱したシステム運用の方法論で、システム運用にソフトウェア・エンジニアリングの視点を取り入れ、プロダクション環境での信頼性を向上させることを目的としています。主に、インシデントの予防や対応、運用の自動化などを通じて、ビジネスやサービスの安定性を支えます。

## SREの役割
　SREの役割は多岐にわたりますが、主に以下の3つの分野に大別されます。

### モニタリングと可観測性の向上
　システムの状態をリアルタイムで監視し、異常を早期に検知するためのモニタリングや可観測性を高める取り組みです。単なる監視ではなく、システム内の隠れた問題を可視化し、必要に応じて適切なアクションを取るための体制を整えます。

### インシデント管理と復旧対応
　インシデント発生時に迅速に復旧するだけでなく、根本原因の特定や、同様の問題が再発しないようなプロアクティブな対応が求められます。SREは、インシデントの予防策と復旧のプロセスを改善し、システムの信頼性を高める役割を担っています。

### 自動化と効率化
　ルーチン作業の自動化や効率化によって、エンジニアの手作業を減らし、再現性と信頼性の向上を図ります。これにより、チームはより複雑で価値の高い課題に集中でき、システム全体の品質とパフォーマンスが向上します。

# SREとLLMの関わり

　SRE業務にLLMを導入することで、アラート分析、オペレーション支援、アクセス制御の自動化など、さまざまな領域で効率化が期待されています。ここでは、具体的に私が日々実践しているLLMの活用方法を紹介します。


## アラートの分析

　オンコール対応時に通知されたアラート内容やログ、バックトレース情報をLLMに入力して分析を行います。最新のLLMモデルでは非常に高い精度で解析が可能で、適切な入力を行えば、アラートの直接原因や問題の切り分けに役立つアドバイスが得られます。これにより、特定の問題の対処方法や原因の推測を迅速に行えるようになり、対応時間の短縮に貢献しています。

## オペレーションの補助

　SRE業務では、Infrastructure as Code（IaC）の普及によりインフラ構成をコード化することが標準化しています。LLMは、このようなインフラ定義の作業にも大いに役立っています。

### Terraform定義などの自動生成

　Terraformは、HashiCorpが提供するインフラ構成管理ツールで、インフラのあるべき状態をHashiCorp Configuration Language（HCL）で宣言的に定義し、`apply`コマンドを実行することでインフラを指定の状態に収束させることが可能です。HCLはプログラミング言語に比べシンプルで記号データとしての表現力が限られているため、公開されているサンプルや定義パターンが多く、LLMを用いた自動生成との相性が非常に良いのが特徴です。

　また、`terraform plan`コマンドによって、現在の環境と定義内容の差分を確認できるため、LLMが生成したコードの適用前に変更内容を検証でき、安全性も確保しやすくなっています。これにより、反復的なオペレーションやコードレビューの効率が向上し、SREチームがより複雑なタスクに集中できる環境が整っています。

### afaの紹介

　私の同僚である[@monochromegane](https://x.com/monochromegane)が開発している [afa](https://github.com/monochromegane/afa)について紹介します。afaはコマンドラインで利用可能なAIクライアントで、入力された情報をもとにAIを活用したオペレーションを実行します。たとえば、「カレントディレクトリ配下のtxtファイルの一覧がほしい」といった指示に対し、`find . -name *.txt` というコマンドが返されるイメージです。これは内部でOpenAIの`Structured Output`を利用しており、利用者の工夫次第でさまざまな操作をAIと組み合わせて行うことが可能です。

## 技術の影響範囲の拡張

　ここでは、私自身の実体験から、LLMによる技術の影響範囲の拡張についてお話しします。

### LLMを活用した機械学習によるアクセス制御

　以前、機械学習に関心を持ち、Pythonや数学の技術書を手に学習を進めていましたが、言語や数学への苦手意識、そして実際のアイデア不足もあり、長らく運用サービスで機械学習を活用する機会はありませんでした。しかし、LLMの登場により、機械学習の学習用コードやAPI実装を自動生成できるようになったことで、今年、MLをベースとしたボット判定アルゴリズムを開発するに至りました。

　この仕組みをリリースできたのは、以前から学んでいた「特徴量」の考え方やエンジニアリングの基礎、そしてシステム知識が備わっていたからです。LLMの支援によってアイデアを形にするハードルが大きく下がり、最終的にこのアルゴリズムを完成させることができました。

　このように、LLMの活用により知識があっても実践が難しかった分野も実践に移しやすくなり、エンジニアが新たな技術領域に挑戦するきっかけを得られるようになっています。

# エンジニアの生存戦略
## どのような仕事が残るか？
### 出力の真贋を見極める
### LLMのグルー実装としての人類
## LLMが今はまだあまり向かないこと
### バリデーション
### 決定となること
## この先LLMはどうなっていくか
### プラットフォームとしてのLLM
### ローカル実行されるLLM
## この先、話し手はどうしていくか
### 継続的な基礎知識の研鑽
### プラットフォームとしてのLLM活用
### 情報の真贋を見る力を養う
